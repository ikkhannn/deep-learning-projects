{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv(\"iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "le = LabelEncoder()\n",
    "Y=le.fit_transform(dataset[:,4])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle= True)\n",
    "\n",
    "one_hot_train_labels=to_categorical(y_train)\n",
    "one_hot_test_labels=to_categorical(y_test)\n",
    "\n",
    "x_val=x_train[:25]\n",
    "partial_x_train=x_train[25:]\n",
    "\n",
    "y_val=one_hot_train_labels[:25]\n",
    "partial_y_train=one_hot_train_labels[25:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=200, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Baseline: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 91.33% (11.18%)\n"
     ]
    }
   ],
   "source": [
    "def create_small():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(4,activation='relu',input_shape=(4,)))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
    "    return model\n",
    "estimator = KerasClassifier(build_fn=create_small, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
    "    return model\n",
    "estimator = KerasClassifier(build_fn=create_large, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overfit():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(64,activation='relu',input_shape=(4,)))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
    "    return model\n",
    "estimator = KerasClassifier(build_fn=create_overfit, epochs=300, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e21f188c5381>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Baseline: %.2f%% (%.2f%%)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 231\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_small():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
    "    return model\n",
    "estimator = KerasClassifier(build_fn=create_small, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\me\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 75 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1066 - acc: 0.5733 - val_loss: 0.9433 - val_acc: 0.6800\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 1.0036 - acc: 0.6667 - val_loss: 0.9132 - val_acc: 0.7200\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.9545 - acc: 0.6667 - val_loss: 0.8785 - val_acc: 0.7200\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.9069 - acc: 0.6667 - val_loss: 0.8573 - val_acc: 0.7200\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.8742 - acc: 0.6667 - val_loss: 0.8448 - val_acc: 0.7200\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.8525 - acc: 0.6667 - val_loss: 0.8302 - val_acc: 0.7200\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.8333 - acc: 0.6667 - val_loss: 0.8185 - val_acc: 0.7200\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.8118 - acc: 0.6667 - val_loss: 0.8093 - val_acc: 0.7200\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.7913 - acc: 0.6667 - val_loss: 0.8004 - val_acc: 0.7200\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7729 - acc: 0.6933 - val_loss: 0.7924 - val_acc: 0.8000\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.7543 - acc: 0.8133 - val_loss: 0.7842 - val_acc: 0.8400\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.7377 - acc: 0.8400 - val_loss: 0.7748 - val_acc: 0.8000\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7226 - acc: 0.8267 - val_loss: 0.7675 - val_acc: 0.6400\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.7077 - acc: 0.7867 - val_loss: 0.7601 - val_acc: 0.5600\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.6955 - acc: 0.7467 - val_loss: 0.7525 - val_acc: 0.5600\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6821 - acc: 0.7333 - val_loss: 0.7462 - val_acc: 0.5600\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.6698 - acc: 0.7333 - val_loss: 0.7471 - val_acc: 0.5600\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.6572 - acc: 0.7200 - val_loss: 0.7451 - val_acc: 0.5600\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6470 - acc: 0.7200 - val_loss: 0.7355 - val_acc: 0.5600\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6407 - acc: 0.7200 - val_loss: 0.7348 - val_acc: 0.5600\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6302 - acc: 0.7200 - val_loss: 0.7269 - val_acc: 0.5600\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6226 - acc: 0.7200 - val_loss: 0.7146 - val_acc: 0.5600\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6142 - acc: 0.7200 - val_loss: 0.7162 - val_acc: 0.5600\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6067 - acc: 0.7200 - val_loss: 0.7121 - val_acc: 0.5600\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.5987 - acc: 0.7200 - val_loss: 0.7014 - val_acc: 0.5600\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5931 - acc: 0.7200 - val_loss: 0.6922 - val_acc: 0.5600\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5886 - acc: 0.7200 - val_loss: 0.6907 - val_acc: 0.5600\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5794 - acc: 0.7200 - val_loss: 0.6843 - val_acc: 0.5600\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5729 - acc: 0.7200 - val_loss: 0.6822 - val_acc: 0.5600\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5670 - acc: 0.7200 - val_loss: 0.6812 - val_acc: 0.5600\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5611 - acc: 0.7200 - val_loss: 0.6752 - val_acc: 0.5600\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5557 - acc: 0.7200 - val_loss: 0.6644 - val_acc: 0.5600\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.5498 - acc: 0.7200 - val_loss: 0.6543 - val_acc: 0.5600\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5434 - acc: 0.7200 - val_loss: 0.6539 - val_acc: 0.5600\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5369 - acc: 0.7200 - val_loss: 0.6450 - val_acc: 0.5600\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5310 - acc: 0.7200 - val_loss: 0.6417 - val_acc: 0.5600\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.5296 - acc: 0.7200 - val_loss: 0.6309 - val_acc: 0.5600\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5210 - acc: 0.7200 - val_loss: 0.6296 - val_acc: 0.5600\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5164 - acc: 0.7200 - val_loss: 0.6261 - val_acc: 0.5600\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.5112 - acc: 0.7200 - val_loss: 0.6233 - val_acc: 0.5600\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5073 - acc: 0.7200 - val_loss: 0.6250 - val_acc: 0.5600\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5027 - acc: 0.7200 - val_loss: 0.6188 - val_acc: 0.5600\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4998 - acc: 0.7200 - val_loss: 0.6120 - val_acc: 0.5600\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4928 - acc: 0.7200 - val_loss: 0.6033 - val_acc: 0.5600\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4893 - acc: 0.7200 - val_loss: 0.6041 - val_acc: 0.5600\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4839 - acc: 0.7200 - val_loss: 0.6026 - val_acc: 0.5600\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4828 - acc: 0.7200 - val_loss: 0.6006 - val_acc: 0.5600\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4758 - acc: 0.7200 - val_loss: 0.5878 - val_acc: 0.5600\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4710 - acc: 0.7200 - val_loss: 0.5790 - val_acc: 0.5600\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4671 - acc: 0.7200 - val_loss: 0.5667 - val_acc: 0.5600\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4623 - acc: 0.7600 - val_loss: 0.5673 - val_acc: 0.5600\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4616 - acc: 0.7333 - val_loss: 0.5553 - val_acc: 0.5600\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4546 - acc: 0.7600 - val_loss: 0.5577 - val_acc: 0.5600\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4513 - acc: 0.7333 - val_loss: 0.5532 - val_acc: 0.5600\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4470 - acc: 0.7333 - val_loss: 0.5553 - val_acc: 0.5600\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4421 - acc: 0.7333 - val_loss: 0.5419 - val_acc: 0.5600\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4380 - acc: 0.7600 - val_loss: 0.5409 - val_acc: 0.5600\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4339 - acc: 0.7467 - val_loss: 0.5387 - val_acc: 0.5600\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4316 - acc: 0.7333 - val_loss: 0.5278 - val_acc: 0.6800\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4291 - acc: 0.8267 - val_loss: 0.5292 - val_acc: 0.5600\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4242 - acc: 0.7333 - val_loss: 0.5175 - val_acc: 0.7200\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4202 - acc: 0.7867 - val_loss: 0.5096 - val_acc: 0.8400\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4168 - acc: 0.8267 - val_loss: 0.5038 - val_acc: 0.8400\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4137 - acc: 0.8400 - val_loss: 0.4977 - val_acc: 0.8800\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4111 - acc: 0.8667 - val_loss: 0.4911 - val_acc: 0.9600\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s 93us/step - loss: 0.4095 - acc: 0.8800 - val_loss: 0.4877 - val_acc: 0.8800\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4046 - acc: 0.8800 - val_loss: 0.4822 - val_acc: 0.9600\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4030 - acc: 0.9600 - val_loss: 0.4849 - val_acc: 0.8400\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3970 - acc: 0.8533 - val_loss: 0.4796 - val_acc: 0.8800\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3940 - acc: 0.8800 - val_loss: 0.4829 - val_acc: 0.8400\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3914 - acc: 0.8133 - val_loss: 0.4734 - val_acc: 0.8400\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3878 - acc: 0.8533 - val_loss: 0.4674 - val_acc: 0.8800\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3844 - acc: 0.8933 - val_loss: 0.4625 - val_acc: 0.9200\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 93us/step - loss: 0.3809 - acc: 0.8800 - val_loss: 0.4544 - val_acc: 0.9600\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3776 - acc: 0.9733 - val_loss: 0.4556 - val_acc: 0.9200\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3771 - acc: 0.9333 - val_loss: 0.4475 - val_acc: 0.9600\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3735 - acc: 0.9333 - val_loss: 0.4401 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3694 - acc: 0.9733 - val_loss: 0.4418 - val_acc: 0.9200\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3654 - acc: 0.9467 - val_loss: 0.4430 - val_acc: 0.9200\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3621 - acc: 0.8933 - val_loss: 0.4344 - val_acc: 0.9600\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3595 - acc: 0.9733 - val_loss: 0.4294 - val_acc: 0.9600\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3560 - acc: 0.9733 - val_loss: 0.4339 - val_acc: 0.8800\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3533 - acc: 0.8933 - val_loss: 0.4304 - val_acc: 0.8800\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3544 - acc: 0.8400 - val_loss: 0.4211 - val_acc: 0.9200\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3473 - acc: 0.9600 - val_loss: 0.4242 - val_acc: 0.9200\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3444 - acc: 0.9067 - val_loss: 0.4164 - val_acc: 0.9200\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3432 - acc: 0.9200 - val_loss: 0.4098 - val_acc: 0.9200\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3375 - acc: 0.9733 - val_loss: 0.4060 - val_acc: 0.9200\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3357 - acc: 0.9600 - val_loss: 0.4115 - val_acc: 0.9200\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3332 - acc: 0.9067 - val_loss: 0.3965 - val_acc: 0.9200\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3294 - acc: 0.9733 - val_loss: 0.4013 - val_acc: 0.9200\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3280 - acc: 0.9333 - val_loss: 0.3867 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3276 - acc: 0.9733 - val_loss: 0.3852 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3212 - acc: 0.9733 - val_loss: 0.3838 - val_acc: 0.9600\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3229 - acc: 0.9733 - val_loss: 0.3817 - val_acc: 0.9600\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3217 - acc: 0.9600 - val_loss: 0.3823 - val_acc: 0.9200\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3147 - acc: 0.9600 - val_loss: 0.3714 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3117 - acc: 0.9733 - val_loss: 0.3669 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3092 - acc: 0.9733 - val_loss: 0.3620 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3063 - acc: 0.9733 - val_loss: 0.3622 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b1710ca48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "inputs= keras.Input(shape=(4,))\n",
    "x=Dense(8,activation='relu')(inputs)\n",
    "x=Dense(8,activation='relu')(x)\n",
    "outputs=Dense(3,activation='softmax')(x) \n",
    "model=keras.Model(inputs,outputs)\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])    \n",
    "model.fit(partial_x_train,partial_y_train,epochs=100,validation_data=(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75 samples, validate on 25 samples\n",
      "Epoch 1/200\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.2175 - acc: 0.3867 - val_loss: 1.1803 - val_acc: 0.2800\n",
      "Epoch 2/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 1.1690 - acc: 0.3867 - val_loss: 1.1552 - val_acc: 0.2800\n",
      "Epoch 3/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 1.1408 - acc: 0.3867 - val_loss: 1.1403 - val_acc: 0.2800\n",
      "Epoch 4/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 1.1183 - acc: 0.3867 - val_loss: 1.1179 - val_acc: 0.2800\n",
      "Epoch 5/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 1.0939 - acc: 0.3867 - val_loss: 1.0958 - val_acc: 0.2800\n",
      "Epoch 6/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 1.0712 - acc: 0.4000 - val_loss: 1.0738 - val_acc: 0.2800\n",
      "Epoch 7/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 1.0509 - acc: 0.4133 - val_loss: 1.0552 - val_acc: 0.3600\n",
      "Epoch 8/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 1.0306 - acc: 0.4533 - val_loss: 1.0362 - val_acc: 0.4800\n",
      "Epoch 9/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 1.0092 - acc: 0.5333 - val_loss: 1.0178 - val_acc: 0.5200\n",
      "Epoch 10/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.9876 - acc: 0.6000 - val_loss: 1.0009 - val_acc: 0.6000\n",
      "Epoch 11/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.9673 - acc: 0.6533 - val_loss: 0.9862 - val_acc: 0.6800\n",
      "Epoch 12/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.9494 - acc: 0.6667 - val_loss: 0.9718 - val_acc: 0.7200\n",
      "Epoch 13/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.9323 - acc: 0.6667 - val_loss: 0.9577 - val_acc: 0.7200\n",
      "Epoch 14/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.9146 - acc: 0.6667 - val_loss: 0.9455 - val_acc: 0.7200\n",
      "Epoch 15/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.8979 - acc: 0.6667 - val_loss: 0.9334 - val_acc: 0.6800\n",
      "Epoch 16/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.8826 - acc: 0.6533 - val_loss: 0.9242 - val_acc: 0.5200\n",
      "Epoch 17/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.8681 - acc: 0.6133 - val_loss: 0.9130 - val_acc: 0.5200\n",
      "Epoch 18/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.8560 - acc: 0.6133 - val_loss: 0.9037 - val_acc: 0.5200\n",
      "Epoch 19/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.8432 - acc: 0.5867 - val_loss: 0.8936 - val_acc: 0.4400\n",
      "Epoch 20/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.8304 - acc: 0.5333 - val_loss: 0.8861 - val_acc: 0.4000\n",
      "Epoch 21/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.8184 - acc: 0.5333 - val_loss: 0.8766 - val_acc: 0.3600\n",
      "Epoch 22/200\n",
      "75/75 [==============================] - 0s 93us/step - loss: 0.8065 - acc: 0.4667 - val_loss: 0.8686 - val_acc: 0.3600\n",
      "Epoch 23/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.7967 - acc: 0.4933 - val_loss: 0.8593 - val_acc: 0.4000\n",
      "Epoch 24/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.7858 - acc: 0.5333 - val_loss: 0.8525 - val_acc: 0.5600\n",
      "Epoch 25/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.7762 - acc: 0.7067 - val_loss: 0.8453 - val_acc: 0.5200\n",
      "Epoch 26/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7677 - acc: 0.6533 - val_loss: 0.8387 - val_acc: 0.5600\n",
      "Epoch 27/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.7583 - acc: 0.7200 - val_loss: 0.8335 - val_acc: 0.5600\n",
      "Epoch 28/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7495 - acc: 0.7200 - val_loss: 0.8271 - val_acc: 0.5600\n",
      "Epoch 29/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7402 - acc: 0.7200 - val_loss: 0.8207 - val_acc: 0.5600\n",
      "Epoch 30/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7323 - acc: 0.7200 - val_loss: 0.8142 - val_acc: 0.5600\n",
      "Epoch 31/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7237 - acc: 0.7200 - val_loss: 0.8088 - val_acc: 0.5600\n",
      "Epoch 32/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.7155 - acc: 0.7200 - val_loss: 0.8022 - val_acc: 0.5600\n",
      "Epoch 33/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.7072 - acc: 0.7200 - val_loss: 0.7960 - val_acc: 0.5600\n",
      "Epoch 34/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6991 - acc: 0.7200 - val_loss: 0.7907 - val_acc: 0.5600\n",
      "Epoch 35/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6916 - acc: 0.7200 - val_loss: 0.7821 - val_acc: 0.5600\n",
      "Epoch 36/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6831 - acc: 0.7200 - val_loss: 0.7758 - val_acc: 0.5600\n",
      "Epoch 37/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.6758 - acc: 0.7200 - val_loss: 0.7669 - val_acc: 0.5600\n",
      "Epoch 38/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6688 - acc: 0.7200 - val_loss: 0.7605 - val_acc: 0.5600\n",
      "Epoch 39/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6613 - acc: 0.7200 - val_loss: 0.7550 - val_acc: 0.5600\n",
      "Epoch 40/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.6539 - acc: 0.7200 - val_loss: 0.7484 - val_acc: 0.5600\n",
      "Epoch 41/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6464 - acc: 0.7200 - val_loss: 0.7415 - val_acc: 0.5600\n",
      "Epoch 42/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6394 - acc: 0.7200 - val_loss: 0.7357 - val_acc: 0.5600\n",
      "Epoch 43/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.6336 - acc: 0.7200 - val_loss: 0.7315 - val_acc: 0.5600\n",
      "Epoch 44/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6260 - acc: 0.7200 - val_loss: 0.7259 - val_acc: 0.5600\n",
      "Epoch 45/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6193 - acc: 0.7200 - val_loss: 0.7171 - val_acc: 0.5600\n",
      "Epoch 46/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.6127 - acc: 0.7200 - val_loss: 0.7109 - val_acc: 0.5600\n",
      "Epoch 47/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6063 - acc: 0.7200 - val_loss: 0.7058 - val_acc: 0.5600\n",
      "Epoch 48/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.6004 - acc: 0.7200 - val_loss: 0.6987 - val_acc: 0.5600\n",
      "Epoch 49/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5944 - acc: 0.7200 - val_loss: 0.6959 - val_acc: 0.5600\n",
      "Epoch 50/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5878 - acc: 0.7200 - val_loss: 0.6906 - val_acc: 0.5600\n",
      "Epoch 51/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5817 - acc: 0.7200 - val_loss: 0.6879 - val_acc: 0.5600\n",
      "Epoch 52/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5759 - acc: 0.7200 - val_loss: 0.6809 - val_acc: 0.5600\n",
      "Epoch 53/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.5702 - acc: 0.7200 - val_loss: 0.6745 - val_acc: 0.5600\n",
      "Epoch 54/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5646 - acc: 0.7200 - val_loss: 0.6724 - val_acc: 0.5600\n",
      "Epoch 55/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5594 - acc: 0.7200 - val_loss: 0.6698 - val_acc: 0.5600\n",
      "Epoch 56/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5535 - acc: 0.7200 - val_loss: 0.6657 - val_acc: 0.5600\n",
      "Epoch 57/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5502 - acc: 0.7200 - val_loss: 0.6604 - val_acc: 0.5600\n",
      "Epoch 58/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5433 - acc: 0.7200 - val_loss: 0.6556 - val_acc: 0.5600\n",
      "Epoch 59/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5383 - acc: 0.7200 - val_loss: 0.6485 - val_acc: 0.5600\n",
      "Epoch 60/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5331 - acc: 0.7200 - val_loss: 0.6430 - val_acc: 0.5600\n",
      "Epoch 61/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.5287 - acc: 0.7200 - val_loss: 0.6404 - val_acc: 0.5600\n",
      "Epoch 62/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5247 - acc: 0.7200 - val_loss: 0.6345 - val_acc: 0.5600\n",
      "Epoch 63/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.5195 - acc: 0.7200 - val_loss: 0.6274 - val_acc: 0.5600\n",
      "Epoch 64/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5148 - acc: 0.7200 - val_loss: 0.6237 - val_acc: 0.5600\n",
      "Epoch 65/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5106 - acc: 0.7200 - val_loss: 0.6218 - val_acc: 0.5600\n",
      "Epoch 66/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5062 - acc: 0.7200 - val_loss: 0.6197 - val_acc: 0.5600\n",
      "Epoch 67/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.5019 - acc: 0.7200 - val_loss: 0.6168 - val_acc: 0.5600\n",
      "Epoch 68/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4977 - acc: 0.7200 - val_loss: 0.6127 - val_acc: 0.5600\n",
      "Epoch 69/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4934 - acc: 0.7200 - val_loss: 0.6083 - val_acc: 0.5600\n",
      "Epoch 70/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4894 - acc: 0.7200 - val_loss: 0.6044 - val_acc: 0.5600\n",
      "Epoch 71/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4851 - acc: 0.7200 - val_loss: 0.6026 - val_acc: 0.5600\n",
      "Epoch 72/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4817 - acc: 0.7200 - val_loss: 0.5953 - val_acc: 0.5600\n",
      "Epoch 73/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4781 - acc: 0.7200 - val_loss: 0.5922 - val_acc: 0.5600\n",
      "Epoch 74/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4739 - acc: 0.7200 - val_loss: 0.5878 - val_acc: 0.5600\n",
      "Epoch 75/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4701 - acc: 0.7200 - val_loss: 0.5821 - val_acc: 0.5600\n",
      "Epoch 76/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4666 - acc: 0.7200 - val_loss: 0.5802 - val_acc: 0.5600\n",
      "Epoch 77/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4632 - acc: 0.7200 - val_loss: 0.5772 - val_acc: 0.5600\n",
      "Epoch 78/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4598 - acc: 0.7200 - val_loss: 0.5699 - val_acc: 0.5600\n",
      "Epoch 79/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4569 - acc: 0.7200 - val_loss: 0.5695 - val_acc: 0.5600\n",
      "Epoch 80/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4534 - acc: 0.7200 - val_loss: 0.5631 - val_acc: 0.5600\n",
      "Epoch 81/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4501 - acc: 0.7200 - val_loss: 0.5591 - val_acc: 0.5600\n",
      "Epoch 82/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4471 - acc: 0.7200 - val_loss: 0.5579 - val_acc: 0.5600\n",
      "Epoch 83/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4439 - acc: 0.7200 - val_loss: 0.5568 - val_acc: 0.5600\n",
      "Epoch 84/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4410 - acc: 0.7200 - val_loss: 0.5549 - val_acc: 0.5600\n",
      "Epoch 85/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4393 - acc: 0.7200 - val_loss: 0.5484 - val_acc: 0.5600\n",
      "Epoch 86/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4350 - acc: 0.7200 - val_loss: 0.5450 - val_acc: 0.5600\n",
      "Epoch 87/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4323 - acc: 0.7200 - val_loss: 0.5424 - val_acc: 0.5600\n",
      "Epoch 88/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4289 - acc: 0.7200 - val_loss: 0.5392 - val_acc: 0.5600\n",
      "Epoch 89/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4267 - acc: 0.7200 - val_loss: 0.5388 - val_acc: 0.5600\n",
      "Epoch 90/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4237 - acc: 0.7200 - val_loss: 0.5344 - val_acc: 0.5600\n",
      "Epoch 91/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4207 - acc: 0.7200 - val_loss: 0.5285 - val_acc: 0.5600\n",
      "Epoch 92/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4182 - acc: 0.7200 - val_loss: 0.5209 - val_acc: 0.5600\n",
      "Epoch 93/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4154 - acc: 0.7333 - val_loss: 0.5192 - val_acc: 0.5600\n",
      "Epoch 94/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.4119 - acc: 0.7200 - val_loss: 0.5180 - val_acc: 0.5600\n",
      "Epoch 95/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4095 - acc: 0.7200 - val_loss: 0.5137 - val_acc: 0.5600\n",
      "Epoch 96/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.4066 - acc: 0.7200 - val_loss: 0.5095 - val_acc: 0.5600\n",
      "Epoch 97/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.4042 - acc: 0.7333 - val_loss: 0.5077 - val_acc: 0.5600\n",
      "Epoch 98/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.4013 - acc: 0.7333 - val_loss: 0.5002 - val_acc: 0.6000\n",
      "Epoch 99/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3987 - acc: 0.7467 - val_loss: 0.4988 - val_acc: 0.5600\n",
      "Epoch 100/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3963 - acc: 0.7467 - val_loss: 0.4997 - val_acc: 0.5600\n",
      "Epoch 101/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3954 - acc: 0.7333 - val_loss: 0.4992 - val_acc: 0.5600\n",
      "Epoch 102/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3942 - acc: 0.7333 - val_loss: 0.4933 - val_acc: 0.5600\n",
      "Epoch 103/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3897 - acc: 0.7467 - val_loss: 0.4875 - val_acc: 0.6000\n",
      "Epoch 104/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3865 - acc: 0.7600 - val_loss: 0.4856 - val_acc: 0.6000\n",
      "Epoch 105/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3841 - acc: 0.7467 - val_loss: 0.4813 - val_acc: 0.6800\n",
      "Epoch 106/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3817 - acc: 0.7733 - val_loss: 0.4776 - val_acc: 0.7200\n",
      "Epoch 107/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3791 - acc: 0.7733 - val_loss: 0.4750 - val_acc: 0.7200\n",
      "Epoch 108/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3764 - acc: 0.7600 - val_loss: 0.4677 - val_acc: 0.8000\n",
      "Epoch 109/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3738 - acc: 0.7733 - val_loss: 0.4607 - val_acc: 0.8400\n",
      "Epoch 110/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3717 - acc: 0.8133 - val_loss: 0.4555 - val_acc: 0.8800\n",
      "Epoch 111/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3702 - acc: 0.8133 - val_loss: 0.4538 - val_acc: 0.8400\n",
      "Epoch 112/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3683 - acc: 0.8133 - val_loss: 0.4555 - val_acc: 0.8400\n",
      "Epoch 113/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3650 - acc: 0.7867 - val_loss: 0.4555 - val_acc: 0.8000\n",
      "Epoch 114/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3641 - acc: 0.8000 - val_loss: 0.4536 - val_acc: 0.8000\n",
      "Epoch 115/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3608 - acc: 0.7867 - val_loss: 0.4507 - val_acc: 0.8000\n",
      "Epoch 116/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3596 - acc: 0.7733 - val_loss: 0.4484 - val_acc: 0.8000\n",
      "Epoch 117/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3567 - acc: 0.7867 - val_loss: 0.4424 - val_acc: 0.8400\n",
      "Epoch 118/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3543 - acc: 0.8133 - val_loss: 0.4360 - val_acc: 0.8800\n",
      "Epoch 119/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3532 - acc: 0.8400 - val_loss: 0.4307 - val_acc: 0.9200\n",
      "Epoch 120/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3498 - acc: 0.9067 - val_loss: 0.4279 - val_acc: 0.9200\n",
      "Epoch 121/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3507 - acc: 0.8267 - val_loss: 0.4276 - val_acc: 0.8800\n",
      "Epoch 122/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.3458 - acc: 0.8800 - val_loss: 0.4258 - val_acc: 0.8800\n",
      "Epoch 123/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 67us/step - loss: 0.3435 - acc: 0.8267 - val_loss: 0.4188 - val_acc: 0.9200\n",
      "Epoch 124/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3421 - acc: 0.9200 - val_loss: 0.4137 - val_acc: 0.9600\n",
      "Epoch 125/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3396 - acc: 0.9600 - val_loss: 0.4101 - val_acc: 0.9600\n",
      "Epoch 126/200\n",
      "75/75 [==============================] - 0s 93us/step - loss: 0.3372 - acc: 0.9733 - val_loss: 0.4119 - val_acc: 0.9200\n",
      "Epoch 127/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.3360 - acc: 0.9467 - val_loss: 0.4120 - val_acc: 0.8800\n",
      "Epoch 128/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3334 - acc: 0.8667 - val_loss: 0.4069 - val_acc: 0.9200\n",
      "Epoch 129/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3313 - acc: 0.9067 - val_loss: 0.4016 - val_acc: 0.9600\n",
      "Epoch 130/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3303 - acc: 0.9200 - val_loss: 0.4028 - val_acc: 0.9200\n",
      "Epoch 131/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3289 - acc: 0.8667 - val_loss: 0.3980 - val_acc: 0.9200\n",
      "Epoch 132/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3250 - acc: 0.9600 - val_loss: 0.3992 - val_acc: 0.9200\n",
      "Epoch 133/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3235 - acc: 0.9067 - val_loss: 0.3910 - val_acc: 0.9600\n",
      "Epoch 134/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3211 - acc: 0.9733 - val_loss: 0.3911 - val_acc: 0.9200\n",
      "Epoch 135/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3197 - acc: 0.9600 - val_loss: 0.3881 - val_acc: 0.9600\n",
      "Epoch 136/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3194 - acc: 0.9733 - val_loss: 0.3900 - val_acc: 0.9200\n",
      "Epoch 137/200\n",
      "75/75 [==============================] - 0s 71us/step - loss: 0.3162 - acc: 0.9067 - val_loss: 0.3824 - val_acc: 0.9600\n",
      "Epoch 138/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.3144 - acc: 0.9733 - val_loss: 0.3770 - val_acc: 0.9600\n",
      "Epoch 139/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.3125 - acc: 0.9733 - val_loss: 0.3768 - val_acc: 0.9600\n",
      "Epoch 140/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3103 - acc: 0.9733 - val_loss: 0.3750 - val_acc: 0.9600\n",
      "Epoch 141/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.3087 - acc: 0.9733 - val_loss: 0.3751 - val_acc: 0.9600\n",
      "Epoch 142/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3067 - acc: 0.9600 - val_loss: 0.3708 - val_acc: 0.9600\n",
      "Epoch 143/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3046 - acc: 0.9733 - val_loss: 0.3636 - val_acc: 0.9600\n",
      "Epoch 144/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3027 - acc: 0.9733 - val_loss: 0.3659 - val_acc: 0.9600\n",
      "Epoch 145/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3008 - acc: 0.9733 - val_loss: 0.3667 - val_acc: 0.9200\n",
      "Epoch 146/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.3006 - acc: 0.9200 - val_loss: 0.3604 - val_acc: 0.9600\n",
      "Epoch 147/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.3008 - acc: 0.9600 - val_loss: 0.3597 - val_acc: 0.9600\n",
      "Epoch 148/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2968 - acc: 0.9733 - val_loss: 0.3622 - val_acc: 0.9200\n",
      "Epoch 149/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2947 - acc: 0.9467 - val_loss: 0.3537 - val_acc: 0.9600\n",
      "Epoch 150/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2940 - acc: 0.9600 - val_loss: 0.3496 - val_acc: 0.9600\n",
      "Epoch 151/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2904 - acc: 0.9733 - val_loss: 0.3498 - val_acc: 0.9600\n",
      "Epoch 152/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.2891 - acc: 0.9733 - val_loss: 0.3441 - val_acc: 0.9600\n",
      "Epoch 153/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2880 - acc: 0.9733 - val_loss: 0.3434 - val_acc: 0.9600\n",
      "Epoch 154/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2852 - acc: 0.9733 - val_loss: 0.3382 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2832 - acc: 0.9733 - val_loss: 0.3364 - val_acc: 0.9600\n",
      "Epoch 156/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2819 - acc: 0.9733 - val_loss: 0.3354 - val_acc: 0.9600\n",
      "Epoch 157/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2795 - acc: 0.9733 - val_loss: 0.3280 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2788 - acc: 0.9733 - val_loss: 0.3326 - val_acc: 0.9600\n",
      "Epoch 159/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2766 - acc: 0.9733 - val_loss: 0.3238 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2746 - acc: 0.9733 - val_loss: 0.3272 - val_acc: 0.9600\n",
      "Epoch 161/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2752 - acc: 0.9733 - val_loss: 0.3285 - val_acc: 0.9600\n",
      "Epoch 162/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2722 - acc: 0.9733 - val_loss: 0.3273 - val_acc: 0.9600\n",
      "Epoch 163/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2705 - acc: 0.9733 - val_loss: 0.3231 - val_acc: 0.9600\n",
      "Epoch 164/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2716 - acc: 0.9733 - val_loss: 0.3171 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2677 - acc: 0.9733 - val_loss: 0.3206 - val_acc: 0.9600\n",
      "Epoch 166/200\n",
      "75/75 [==============================] - 0s 93us/step - loss: 0.2661 - acc: 0.9733 - val_loss: 0.3170 - val_acc: 0.9600\n",
      "Epoch 167/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2645 - acc: 0.9733 - val_loss: 0.3097 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2632 - acc: 0.9733 - val_loss: 0.3118 - val_acc: 0.9600\n",
      "Epoch 169/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2615 - acc: 0.9733 - val_loss: 0.3075 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2610 - acc: 0.9733 - val_loss: 0.3084 - val_acc: 0.9600\n",
      "Epoch 171/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2598 - acc: 0.9733 - val_loss: 0.3069 - val_acc: 0.9600\n",
      "Epoch 172/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2590 - acc: 0.9733 - val_loss: 0.3057 - val_acc: 0.9600\n",
      "Epoch 173/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2568 - acc: 0.9733 - val_loss: 0.3032 - val_acc: 0.9600\n",
      "Epoch 174/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2546 - acc: 0.9733 - val_loss: 0.3022 - val_acc: 0.9600\n",
      "Epoch 175/200\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2093 - acc: 1.000 - 0s 67us/step - loss: 0.2558 - acc: 0.9733 - val_loss: 0.3039 - val_acc: 0.9600\n",
      "Epoch 176/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2529 - acc: 0.9733 - val_loss: 0.3026 - val_acc: 0.9600\n",
      "Epoch 177/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2533 - acc: 0.9733 - val_loss: 0.2990 - val_acc: 0.9600\n",
      "Epoch 178/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2507 - acc: 0.9733 - val_loss: 0.2983 - val_acc: 0.9600\n",
      "Epoch 179/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2484 - acc: 0.9733 - val_loss: 0.2920 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2474 - acc: 0.9733 - val_loss: 0.2945 - val_acc: 0.9600\n",
      "Epoch 181/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2458 - acc: 0.9733 - val_loss: 0.2889 - val_acc: 0.9600\n",
      "Epoch 182/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.2449 - acc: 0.9733 - val_loss: 0.2887 - val_acc: 0.9600\n",
      "Epoch 183/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2450 - acc: 0.9733 - val_loss: 0.2885 - val_acc: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2420 - acc: 0.9733 - val_loss: 0.2858 - val_acc: 0.9600\n",
      "Epoch 185/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.2401 - acc: 0.9733 - val_loss: 0.2817 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2399 - acc: 0.9733 - val_loss: 0.2784 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.2366 - acc: 0.9733 - val_loss: 0.2747 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2349 - acc: 0.9733 - val_loss: 0.2678 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2333 - acc: 0.9867 - val_loss: 0.2715 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.2324 - acc: 0.9733 - val_loss: 0.2720 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2311 - acc: 0.9733 - val_loss: 0.2698 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2293 - acc: 0.9733 - val_loss: 0.2605 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.2291 - acc: 0.9867 - val_loss: 0.2560 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2275 - acc: 0.9867 - val_loss: 0.2542 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2260 - acc: 0.9867 - val_loss: 0.2533 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "75/75 [==============================] - 0s 66us/step - loss: 0.2243 - acc: 0.9867 - val_loss: 0.2530 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "75/75 [==============================] - 0s 80us/step - loss: 0.2224 - acc: 0.9867 - val_loss: 0.2489 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "75/75 [==============================] - 0s 53us/step - loss: 0.2211 - acc: 0.9867 - val_loss: 0.2517 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2204 - acc: 0.9733 - val_loss: 0.2464 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "75/75 [==============================] - 0s 67us/step - loss: 0.2181 - acc: 0.9867 - val_loss: 0.2472 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.dense1=Dense(8,activation='relu')\n",
    "        self.dense2=Dense(8,activation='relu')\n",
    "        self.dense3=Dense(3,activation='softmax')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x=self.dense1(inputs)\n",
    "        x=self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "model=MyModel()\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])    \n",
    "history=model.fit(partial_x_train,partial_y_train,epochs=200,validation_data=(x_val,y_val))\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 2.0752 - acc: 0.1150\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 1.2801 - acc: 0.0973\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 1.0735 - acc: 0.2212\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 1.0288 - acc: 0.3628\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9906 - acc: 0.2920\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9761 - acc: 0.3894\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9582 - acc: 0.3363\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9413 - acc: 0.3363\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9345 - acc: 0.4159\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9210 - acc: 0.4336\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9141 - acc: 0.3894\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9051 - acc: 0.4071\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9004 - acc: 0.4336\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8905 - acc: 0.4513\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8820 - acc: 0.4690\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8735 - acc: 0.5044\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8701 - acc: 0.5133\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8600 - acc: 0.4336\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8497 - acc: 0.5929\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8421 - acc: 0.4690\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8363 - acc: 0.4690\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8271 - acc: 0.5841\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8167 - acc: 0.6283\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8069 - acc: 0.5929\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.7982 - acc: 0.6814\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.7907 - acc: 0.4690\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.7809 - acc: 0.6814\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.7679 - acc: 0.6637\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.7562 - acc: 0.5664\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.7437 - acc: 0.7434\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.7336 - acc: 0.6991\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.7214 - acc: 0.6549\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.7068 - acc: 0.7080\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.6996 - acc: 0.8053\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6816 - acc: 0.7434\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6689 - acc: 0.8230\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6574 - acc: 0.6991\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6452 - acc: 0.7788\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.6262 - acc: 0.8319\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.6115 - acc: 0.8053\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5955 - acc: 0.8230\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5856 - acc: 0.7699\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5666 - acc: 0.8230\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5540 - acc: 0.8496\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5385 - acc: 0.8850\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5249 - acc: 0.9381\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.5126 - acc: 0.9292\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.5019 - acc: 0.9027\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.4828 - acc: 0.9292\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.4830 - acc: 0.9381\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.4649 - acc: 0.9292\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.4499 - acc: 0.9558\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.4372 - acc: 0.9646\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4233 - acc: 0.9646\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4152 - acc: 0.9469\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.4046 - acc: 0.9558\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.3915 - acc: 0.9823\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3908 - acc: 0.9469\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3771 - acc: 0.9469\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.3682 - acc: 0.9558\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.3574 - acc: 0.9469\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3521 - acc: 0.9558\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3445 - acc: 0.9558\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3326 - acc: 0.9646\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3347 - acc: 0.9558\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3208 - acc: 0.9558\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3125 - acc: 0.9646\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3080 - acc: 0.9558\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3038 - acc: 0.9469\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3017 - acc: 0.9469\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2891 - acc: 0.9469\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2848 - acc: 0.9558\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2768 - acc: 0.9735\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2707 - acc: 0.9646\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2641 - acc: 0.9646\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2613 - acc: 0.9558\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2578 - acc: 0.9646\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2513 - acc: 0.9646\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2459 - acc: 0.9646\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2407 - acc: 0.9646\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2266 - acc: 0.9735\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2350 - acc: 0.9735\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2286 - acc: 0.9469\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2288 - acc: 0.9558\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2199 - acc: 0.9646\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2182 - acc: 0.9646\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2201 - acc: 0.9646\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2113 - acc: 0.9735\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2029 - acc: 0.9558\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2006 - acc: 0.9646\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1982 - acc: 0.9558\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1953 - acc: 0.9558\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1917 - acc: 0.9823\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1897 - acc: 0.9558\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1874 - acc: 0.9558\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1846 - acc: 0.9558\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1828 - acc: 0.9646\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1806 - acc: 0.9735\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1784 - acc: 0.9735\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1725 - acc: 0.9558\n",
      "37/37 [==============================] - 0s 5ms/step\n",
      "processing fold # 1\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 1.1959 - acc: 0.3451\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 1.0140 - acc: 0.4425\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9492 - acc: 0.4425\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.9004 - acc: 0.4956\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8507 - acc: 0.6903\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8084 - acc: 0.7699\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.7646 - acc: 0.7699\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.7249 - acc: 0.7699\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6828 - acc: 0.7699\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6456 - acc: 0.7699\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6093 - acc: 0.7699\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.5754 - acc: 0.7699\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.5405 - acc: 0.7699\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.5165 - acc: 0.7699\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4881 - acc: 0.7788\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4676 - acc: 0.7699\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4437 - acc: 0.8053\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4230 - acc: 0.7876\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4066 - acc: 0.8053\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3927 - acc: 0.8053\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3778 - acc: 0.8230\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3684 - acc: 0.8319\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3534 - acc: 0.8496\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3450 - acc: 0.8496\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3357 - acc: 0.8319\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3288 - acc: 0.8761\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3172 - acc: 0.8673\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3131 - acc: 0.8850\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3053 - acc: 0.8761\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2974 - acc: 0.8761\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2958 - acc: 0.8850\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2848 - acc: 0.8938\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2779 - acc: 0.9115\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2720 - acc: 0.9204\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2668 - acc: 0.9027\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2621 - acc: 0.9469\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2540 - acc: 0.9204\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2491 - acc: 0.9469\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2450 - acc: 0.9292\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2420 - acc: 0.9558\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2367 - acc: 0.9381\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2335 - acc: 0.9292\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2325 - acc: 0.9558\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2228 - acc: 0.9646\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2177 - acc: 0.9469\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2139 - acc: 0.9735\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2121 - acc: 0.9646\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2048 - acc: 0.9735\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2022 - acc: 0.9558\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1990 - acc: 0.9735\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1972 - acc: 0.9469\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1947 - acc: 0.9735\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1909 - acc: 0.9735\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1862 - acc: 0.9735\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1809 - acc: 0.9735\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1829 - acc: 0.9735\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1772 - acc: 0.9646\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1715 - acc: 0.9735\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1689 - acc: 0.9735\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1684 - acc: 0.9646\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1635 - acc: 0.9735\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1598 - acc: 0.9823\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1606 - acc: 0.9735\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1548 - acc: 0.9735\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1535 - acc: 0.9735\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1486 - acc: 0.9735\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1481 - acc: 0.9735\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1467 - acc: 0.9735\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1457 - acc: 0.9735\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1398 - acc: 0.9735\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1399 - acc: 0.9823\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1399 - acc: 0.9823\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1361 - acc: 0.9823\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1327 - acc: 0.9646\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1392 - acc: 0.9735\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1307 - acc: 0.9823\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1278 - acc: 0.9735\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1262 - acc: 0.9735\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1254 - acc: 0.9823\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1254 - acc: 0.9823\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1212 - acc: 0.9912\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1220 - acc: 0.9912\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1210 - acc: 0.9735\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1197 - acc: 0.9735\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1183 - acc: 0.9735\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1170 - acc: 0.9823\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1139 - acc: 0.9823\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1106 - acc: 0.9735\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1138 - acc: 0.9823\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1114 - acc: 0.9735\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1103 - acc: 0.9823\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1091 - acc: 0.9735\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1070 - acc: 0.9912\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1047 - acc: 0.9823\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1036 - acc: 0.9912\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1018 - acc: 0.9912\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1064 - acc: 0.9735\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1003 - acc: 0.9735\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1041 - acc: 0.9735\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1023 - acc: 0.9823\n",
      "37/37 [==============================] - 0s 6ms/step\n",
      "processing fold # 2\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 3.1127 - acc: 0.4336\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 1.6750 - acc: 0.6106\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8084 - acc: 0.7080\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5987 - acc: 0.8319\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.5395 - acc: 0.7876\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.4921 - acc: 0.7876\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.4565 - acc: 0.7876\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.4298 - acc: 0.7965\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.4061 - acc: 0.8053\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.3890 - acc: 0.7965\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.3776 - acc: 0.8053\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.3644 - acc: 0.8230\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.3491 - acc: 0.8142\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3398 - acc: 0.7965\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3337 - acc: 0.8496\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3292 - acc: 0.8407\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3158 - acc: 0.8407\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3099 - acc: 0.8673\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3055 - acc: 0.8230\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2999 - acc: 0.8850\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2901 - acc: 0.8584\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2828 - acc: 0.8938\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2830 - acc: 0.8584\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2704 - acc: 0.8938\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2681 - acc: 0.8850\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2582 - acc: 0.9204\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2587 - acc: 0.9204\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2507 - acc: 0.8938\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2485 - acc: 0.9027\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2370 - acc: 0.9204\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2367 - acc: 0.9027\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2273 - acc: 0.9027\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2300 - acc: 0.9381\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2177 - acc: 0.9381\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2141 - acc: 0.9204\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.2109 - acc: 0.9381\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2090 - acc: 0.9115\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1980 - acc: 0.9381\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.2016 - acc: 0.9469\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1924 - acc: 0.9735\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1915 - acc: 0.9381\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1825 - acc: 0.9646\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1794 - acc: 0.9823\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1798 - acc: 0.9558\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1728 - acc: 0.9558\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1699 - acc: 0.9646\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1677 - acc: 0.9558\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1637 - acc: 0.9735\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1557 - acc: 0.9735\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1615 - acc: 0.9646\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1515 - acc: 0.9646\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1530 - acc: 0.9646\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1451 - acc: 0.9646\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1416 - acc: 0.9646\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1460 - acc: 0.9735\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1373 - acc: 0.9735\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1404 - acc: 0.9735\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1367 - acc: 0.9735\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1325 - acc: 0.9646\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1299 - acc: 0.9646\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1265 - acc: 0.9558\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1284 - acc: 0.9735\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1275 - acc: 0.9646\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1206 - acc: 0.9735\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1209 - acc: 0.9735\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1173 - acc: 0.9735\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1154 - acc: 0.9735\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.1149 - acc: 0.9646\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.1174 - acc: 0.9735\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1180 - acc: 0.9646\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1113 - acc: 0.9823\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1094 - acc: 0.9735\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1081 - acc: 0.9735\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1047 - acc: 0.9735\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1057 - acc: 0.9735\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1024 - acc: 0.9735\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1011 - acc: 0.9735\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0967 - acc: 0.9646\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0983 - acc: 0.9735\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0961 - acc: 0.9823\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0945 - acc: 0.9735\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0967 - acc: 0.9735\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0932 - acc: 0.9823\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0955 - acc: 0.9646\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0881 - acc: 0.9823\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0924 - acc: 0.9735\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0897 - acc: 0.9735\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0916 - acc: 0.9735\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0901 - acc: 0.9823\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0905 - acc: 0.9735\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0886 - acc: 0.9823\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0833 - acc: 0.9823\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0854 - acc: 0.9735\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0822 - acc: 0.9823\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0875 - acc: 0.9735\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0850 - acc: 0.9735\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0844 - acc: 0.9823\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0864 - acc: 0.9735\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0797 - acc: 0.9823\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0813 - acc: 0.9735\n",
      "37/37 [==============================] - 0s 6ms/step\n",
      "processing fold # 3\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 1.4450 - acc: 0.4425\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.8609 - acc: 0.6372\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.7732 - acc: 0.8850\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.7225 - acc: 0.8850\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6650 - acc: 0.8850\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.6173 - acc: 0.8850\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.5707 - acc: 0.8850\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.5311 - acc: 0.8850\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4937 - acc: 0.8850\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4623 - acc: 0.8850\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4370 - acc: 0.8850\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4170 - acc: 0.8850\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.4008 - acc: 0.8850\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3761 - acc: 0.8850\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3580 - acc: 0.8850\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3402 - acc: 0.8850\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3391 - acc: 0.8850\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3270 - acc: 0.8850\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3071 - acc: 0.8850\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.3034 - acc: 0.8850\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2931 - acc: 0.8850\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2861 - acc: 0.8850\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2753 - acc: 0.8850\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2737 - acc: 0.8850\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2616 - acc: 0.8850\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2597 - acc: 0.8850\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2511 - acc: 0.8850\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2465 - acc: 0.8850\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2375 - acc: 0.8938\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2296 - acc: 0.9204\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2327 - acc: 0.8850\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2218 - acc: 0.8850\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2224 - acc: 0.9027\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2167 - acc: 0.9027\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2161 - acc: 0.8938\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2135 - acc: 0.8938\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.2020 - acc: 0.9381\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1954 - acc: 0.9027\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1949 - acc: 0.9292\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1917 - acc: 0.9115\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1880 - acc: 0.9027\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1804 - acc: 0.9381\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1800 - acc: 0.9292\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.1732 - acc: 0.9204\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1731 - acc: 0.9381\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.1696 - acc: 0.9292\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.1637 - acc: 0.9381\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.1608 - acc: 0.9735\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1588 - acc: 0.9381\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1550 - acc: 0.9381\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1531 - acc: 0.9469\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1479 - acc: 0.9646\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1505 - acc: 0.9646\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1448 - acc: 0.9558\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1410 - acc: 0.9646\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1362 - acc: 0.9823\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1347 - acc: 0.9558\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1343 - acc: 0.9735\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1347 - acc: 0.9646\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1281 - acc: 0.9735\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1249 - acc: 0.9735\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1255 - acc: 0.9823\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1198 - acc: 0.9735\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1183 - acc: 0.9735\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1162 - acc: 0.9735\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1104 - acc: 0.9823\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1147 - acc: 0.9735\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1117 - acc: 0.9912\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1107 - acc: 0.9646\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1046 - acc: 0.9912\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1060 - acc: 0.9646\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0989 - acc: 0.9823\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1014 - acc: 0.9823\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.1013 - acc: 0.9735\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0989 - acc: 0.9735\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0951 - acc: 0.9823\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0965 - acc: 0.9912\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0925 - acc: 0.9735\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0917 - acc: 0.9823\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0874 - acc: 0.9823\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0918 - acc: 0.9912\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0863 - acc: 0.9823\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0871 - acc: 0.9912\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0838 - acc: 0.9735\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0834 - acc: 0.9823\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0794 - acc: 0.9823\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0811 - acc: 0.9823\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0799 - acc: 0.9823\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0768 - acc: 0.9912\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0741 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0756 - acc: 0.9823\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0728 - acc: 0.9823\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0701 - acc: 0.9823\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0700 - acc: 0.9823\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0770 - acc: 0.9912\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0693 - acc: 0.9823\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0715 - acc: 0.9823\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0673 - acc: 0.9912\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0694 - acc: 0.9912\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0650 - acc: 0.9912\n",
      "37/37 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "num_val_samples=len(X)//k\n",
    "num_epochs=100\n",
    "all_scores=[]\n",
    "\n",
    "def create_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
    "    return model\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = dummy_y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate([X[:i * num_val_samples],X[(i + 1) * num_val_samples:]],axis=0)\n",
    "    partial_train_targets = np.concatenate([dummy_y[:i * num_val_samples],dummy_y[(i + 1) * num_val_samples:]],axis=0)\n",
    "    model = create_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=1)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256756764811438"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores\n",
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
